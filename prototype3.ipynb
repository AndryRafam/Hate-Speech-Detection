{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342385cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import pickle\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c2066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727c9c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!\"@__BrighterDays: I can not just sit up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!&amp;#8220;@selfiequeenbri: cause I'm tired of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" &amp;amp; you might not get ya bitch back &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" @rhythmixx_ :hobbies include: fighting Maria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" Keeks is a bitch she curves everyone \" lol I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" Murda Gang bitch its Gang Land \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\" So hoes that smoke are losers ? \" yea ... go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" bad bitches is the only thing that i like \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" bitch get up off me \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0            0      3            0                   0        3      2   \n",
       "1            1      3            0                   3        0      1   \n",
       "2            2      3            0                   3        0      1   \n",
       "3            3      3            0                   2        1      1   \n",
       "4            4      6            0                   6        0      1   \n",
       "5            5      3            1                   2        0      1   \n",
       "6            6      3            0                   3        0      1   \n",
       "7            7      3            0                   3        0      1   \n",
       "8            8      3            0                   3        0      1   \n",
       "9            9      3            1                   2        0      1   \n",
       "10          10      3            0                   3        0      1   \n",
       "11          11      3            0                   3        0      1   \n",
       "12          12      3            0                   2        1      1   \n",
       "13          13      3            0                   3        0      1   \n",
       "14          14      3            1                   2        0      1   \n",
       "\n",
       "                                                tweet  \n",
       "0   !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1   !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2   !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3   !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4   !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "5   !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
       "6   !!!!!!\"@__BrighterDays: I can not just sit up ...  \n",
       "7   !!!!&#8220;@selfiequeenbri: cause I'm tired of...  \n",
       "8   \" &amp; you might not get ya bitch back &amp; ...  \n",
       "9   \" @rhythmixx_ :hobbies include: fighting Maria...  \n",
       "10  \" Keeks is a bitch she curves everyone \" lol I...  \n",
       "11                 \" Murda Gang bitch its Gang Land \"  \n",
       "12  \" So hoes that smoke are losers ? \" yea ... go...  \n",
       "13      \" bad bitches is the only thing that i like \"  \n",
       "14                            \" bitch get up off me \"  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2b2f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24783\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2990ddfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19190</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>19190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4163</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  tweet\n",
       "class                                                                    \n",
       "0            1430      5            6                   5        5   1430\n",
       "1           19190      5            5                   8        4  19190\n",
       "2            4163      5            4                   5        8   4163"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('class').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d57431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...      2\n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1\n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1\n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1\n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['tweet','class']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4c5505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6633d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "def depure_data(data):\n",
    "    #Removing URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'',data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd76beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!!! RT As a woman you shouldnt complain about cleaning up your house. &amp; as a man you should always take the trash out...', '!!!!! RT boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!', '!!!!!!! RT Dawg!!!! RT You ever fuck a bitch and she start to cry? You be confused as shit', '!!!!!!!!! RT she look like a tranny', '!!!!!!!!!!!!! RT The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;']\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "# Splitting pd.Series to data to list\n",
    "data_to_list = train[\"tweet\"].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "print(list(temp[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973672ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672746b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', 'as', 'woman', 'you', 'shouldnt', 'complain', 'about', 'cleaning', 'up', 'your', 'house', 'amp', 'as', 'man', 'you', 'should', 'always', 'take', 'the', 'trash', 'out'], ['rt', 'boy', 'dats', 'cold', 'tyga', 'dwn', 'bad', 'for', 'cuffin', 'dat', 'hoe', 'in', 'the', 'st', 'place'], ['rt', 'dawg', 'rt', 'you', 'ever', 'fuck', 'bitch', 'and', 'she', 'start', 'to', 'cry', 'you', 'be', 'confused', 'as', 'shit'], ['rt', 'she', 'look', 'like', 'tranny'], ['rt', 'the', 'shit', 'you', 'hear', 'about', 'me', 'might', 'be', 'true', 'or', 'it', 'might', 'be', 'faker', 'than', 'the', 'bitch', 'who', 'told', 'it', 'to', 'ya'], ['the', 'shit', 'just', 'blows', 'me', 'claim', 'you', 'so', 'faithful', 'and', 'down', 'for', 'somebody', 'but', 'still', 'fucking', 'with', 'hoes'], ['can', 'not', 'just', 'sit', 'up', 'and', 'hate', 'on', 'another', 'bitch', 'got', 'too', 'much', 'shit', 'going', 'on'], ['cause', 'im', 'tired', 'of', 'you', 'big', 'bitches', 'coming', 'for', 'us', 'skinny', 'girls'], ['amp', 'you', 'might', 'not', 'get', 'ya', 'bitch', 'back', 'amp', 'thats', 'that'], ['hobbies', 'include', 'fighting', 'mariam', 'bitch']]\n",
      "24783\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(temp))\n",
    "print(data_words[:10])\n",
    "print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9a1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "442fdff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt as woman you shouldnt complain about cleaning up your house amp as man you should always take the trash out', 'rt boy dats cold tyga dwn bad for cuffin dat hoe in the st place', 'rt dawg rt you ever fuck bitch and she start to cry you be confused as shit', 'rt she look like tranny', 'rt the shit you hear about me might be true or it might be faker than the bitch who told it to ya']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f00507",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a6f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24783\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "labels = np.array(train[\"class\"])\n",
    "labels = tf.keras.utils.to_categorical(labels,3,dtype=\"int32\")\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1589b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Sequencing and Splitting\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b055e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    3   47   52]\n",
      " [   0    0    0 ...    3  543  515]\n",
      " [   0    0    0 ...  993   71   41]\n",
      " ...\n",
      " [   0    0    0 ...  341   28  274]\n",
      " [   0    0    0 ... 1826    4 1307]\n",
      " [   0    0    0 ...  171   55  115]]\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 5000 # Consider only the top 5k words\n",
    "max_len = 200 # Consider only the 200 first word of each tweet\n",
    "\n",
    "tokenizer3 = Tokenizer(num_words=max_words)\n",
    "tokenizer3.fit_on_texts(data)\n",
    "sequences = tokenizer3.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "with open('tockenizer3.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer3,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(tweets)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17798401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18587 6196 18587 6196\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweets,labels,random_state=0)\n",
    "print(len(x_train),len(x_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa25aa7",
   "metadata": {},
   "source": [
    "## Build and Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e805449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/40\n",
      "581/581 - 15s - loss: 0.3811 - accuracy: 0.8678 - val_loss: 0.2752 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.90413, saving model to best_model3.hdf5\n",
      "Epoch 2/40\n",
      "581/581 - 12s - loss: 0.2338 - accuracy: 0.9171 - val_loss: 0.2773 - val_accuracy: 0.9051\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.90413 to 0.90510, saving model to best_model3.hdf5\n",
      "Epoch 3/40\n",
      "581/581 - 12s - loss: 0.1879 - accuracy: 0.9343 - val_loss: 0.3095 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.90510\n",
      "Epoch 4/40\n",
      "581/581 - 12s - loss: 0.1587 - accuracy: 0.9456 - val_loss: 0.3292 - val_accuracy: 0.8944\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.90510\n",
      "Epoch 5/40\n",
      "581/581 - 12s - loss: 0.1338 - accuracy: 0.9550 - val_loss: 0.3732 - val_accuracy: 0.8915\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.90510\n",
      "Epoch 6/40\n",
      "581/581 - 12s - loss: 0.1154 - accuracy: 0.9606 - val_loss: 0.4043 - val_accuracy: 0.8917\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.90510\n",
      "Epoch 7/40\n",
      "581/581 - 12s - loss: 0.0982 - accuracy: 0.9658 - val_loss: 0.4117 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.90510\n",
      "Epoch 8/40\n",
      "581/581 - 12s - loss: 0.0850 - accuracy: 0.9694 - val_loss: 0.4770 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.90510\n",
      "Epoch 9/40\n",
      "581/581 - 12s - loss: 0.0724 - accuracy: 0.9740 - val_loss: 0.4931 - val_accuracy: 0.8833\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.90510\n",
      "Epoch 10/40\n",
      "581/581 - 12s - loss: 0.0675 - accuracy: 0.9754 - val_loss: 0.5245 - val_accuracy: 0.8788\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.90510\n",
      "Epoch 11/40\n",
      "581/581 - 13s - loss: 0.0563 - accuracy: 0.9799 - val_loss: 0.5572 - val_accuracy: 0.8749\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.90510\n",
      "Epoch 12/40\n",
      "581/581 - 12s - loss: 0.0518 - accuracy: 0.9812 - val_loss: 0.5816 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.90510\n",
      "Epoch 13/40\n",
      "581/581 - 12s - loss: 0.0476 - accuracy: 0.9820 - val_loss: 0.5789 - val_accuracy: 0.8801\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.90510\n",
      "Epoch 14/40\n",
      "581/581 - 12s - loss: 0.0424 - accuracy: 0.9847 - val_loss: 0.5988 - val_accuracy: 0.8856\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.90510\n",
      "Epoch 15/40\n",
      "581/581 - 12s - loss: 0.0384 - accuracy: 0.9859 - val_loss: 0.6281 - val_accuracy: 0.8825\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.90510\n",
      "Epoch 16/40\n",
      "581/581 - 12s - loss: 0.0345 - accuracy: 0.9871 - val_loss: 0.7025 - val_accuracy: 0.8767\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.90510\n",
      "Epoch 17/40\n",
      "581/581 - 12s - loss: 0.0320 - accuracy: 0.9882 - val_loss: 0.7760 - val_accuracy: 0.8636\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.90510\n",
      "Epoch 18/40\n",
      "581/581 - 12s - loss: 0.0355 - accuracy: 0.9868 - val_loss: 0.6841 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.90510\n",
      "Epoch 19/40\n",
      "581/581 - 12s - loss: 0.0305 - accuracy: 0.9883 - val_loss: 0.7607 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.90510\n",
      "Epoch 20/40\n",
      "581/581 - 12s - loss: 0.0260 - accuracy: 0.9890 - val_loss: 0.7709 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.90510\n",
      "Epoch 21/40\n",
      "581/581 - 12s - loss: 0.0255 - accuracy: 0.9905 - val_loss: 0.7561 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.90510\n",
      "Epoch 22/40\n",
      "581/581 - 12s - loss: 0.0219 - accuracy: 0.9919 - val_loss: 0.8226 - val_accuracy: 0.8822\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90510\n",
      "Epoch 23/40\n",
      "581/581 - 12s - loss: 0.0225 - accuracy: 0.9921 - val_loss: 0.8248 - val_accuracy: 0.8744\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90510\n",
      "Epoch 24/40\n",
      "581/581 - 12s - loss: 0.0219 - accuracy: 0.9914 - val_loss: 0.8211 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90510\n",
      "Epoch 25/40\n",
      "581/581 - 12s - loss: 0.0207 - accuracy: 0.9924 - val_loss: 0.8211 - val_accuracy: 0.8754\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90510\n",
      "Epoch 26/40\n",
      "581/581 - 12s - loss: 0.0232 - accuracy: 0.9911 - val_loss: 0.8370 - val_accuracy: 0.8760\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.90510\n",
      "Epoch 27/40\n",
      "581/581 - 13s - loss: 0.0203 - accuracy: 0.9923 - val_loss: 0.8161 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.90510\n",
      "Epoch 28/40\n",
      "581/581 - 13s - loss: 0.0164 - accuracy: 0.9935 - val_loss: 0.8906 - val_accuracy: 0.8773\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.90510\n",
      "Epoch 29/40\n",
      "581/581 - 13s - loss: 0.0182 - accuracy: 0.9926 - val_loss: 0.8724 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90510\n",
      "Epoch 30/40\n",
      "581/581 - 12s - loss: 0.0184 - accuracy: 0.9927 - val_loss: 0.8767 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.90510\n",
      "Epoch 31/40\n",
      "581/581 - 12s - loss: 0.0166 - accuracy: 0.9929 - val_loss: 0.8777 - val_accuracy: 0.8760\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.90510\n",
      "Epoch 32/40\n",
      "581/581 - 12s - loss: 0.0132 - accuracy: 0.9940 - val_loss: 0.8910 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.90510\n",
      "Epoch 33/40\n",
      "581/581 - 12s - loss: 0.0138 - accuracy: 0.9941 - val_loss: 0.9407 - val_accuracy: 0.8751\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.90510\n",
      "Epoch 34/40\n",
      "581/581 - 12s - loss: 0.0131 - accuracy: 0.9943 - val_loss: 0.9848 - val_accuracy: 0.8702\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.90510\n",
      "Epoch 35/40\n",
      "581/581 - 12s - loss: 0.0147 - accuracy: 0.9946 - val_loss: 0.9166 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.90510\n",
      "Epoch 36/40\n",
      "581/581 - 12s - loss: 0.0137 - accuracy: 0.9946 - val_loss: 0.9408 - val_accuracy: 0.8754\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.90510\n",
      "Epoch 37/40\n",
      "581/581 - 12s - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.9832 - val_accuracy: 0.8785\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.90510\n",
      "Epoch 38/40\n",
      "581/581 - 12s - loss: 0.0126 - accuracy: 0.9950 - val_loss: 0.9966 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.90510\n",
      "Epoch 39/40\n",
      "581/581 - 12s - loss: 0.0128 - accuracy: 0.9953 - val_loss: 0.9639 - val_accuracy: 0.8748\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.90510\n",
      "Epoch 40/40\n",
      "581/581 - 12s - loss: 0.0106 - accuracy: 0.9957 - val_loss: 0.9713 - val_accuracy: 0.8759\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.90510\n"
     ]
    }
   ],
   "source": [
    "# BiDR LST LAYER MODEL\n",
    "model = Sequential([\n",
    "    layers.Embedding(max_words,128,input_length=max_len),\n",
    "    #layers.Bidirectional(layers.LSTM(64,return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(64,dropout=0.4)),\n",
    "    layers.Dense(3,activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "checkpoint = ModelCheckpoint(\"best_model3.hdf5\",monitor=\"val_accuracy\",verbose=1,save_best_only=True,mode='auto',period=1,save_weights_only=False)\n",
    "history = model.fit(x_train, y_train, epochs=40, validation_data=(x_test,y_test), verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863a949",
   "metadata": {},
   "source": [
    "## Test the model on dataset X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3178073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 - 2s - loss: 0.2773 - accuracy: 0.9051\n",
      "Model accuracy: 90.51%\n",
      "Score: 95.99 %\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"best_model3.hdf5\")\n",
    "test_loss, test_acc = best_model.evaluate(x_test,y_test,verbose=2)\n",
    "print(\"Model accuracy: {:.2f}%\".format(100*test_acc))\n",
    "predictions = best_model.predict(x_test)\n",
    "score = predictions[0]\n",
    "print(\"Score: {:.2f} %\".format(100*np.max(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f03688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
