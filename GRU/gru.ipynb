{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d7c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18b5d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt as a woman you shouldn complain about cleaning up your house amp as a man you should always take the trash out', 'rt boy dats cold tyga dwn bad for cuffin dat hoe in the place', 'rt dawg rt you ever fuck a bitch and she start to cry you be confused as shit', 'rt she look like a tranny', 'rt the shit you hear about me might be true or it might be faker than the bitch who told it to ya']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"hate_speech.csv\")\n",
    "\n",
    "def preprocess(x):\n",
    "\tx = x.lower()\n",
    "\tx = x.encode(\"ascii\",\"ignore\").decode()\n",
    "\tx = re.sub(\"https*\\S+\",\" \",x)\n",
    "\tx = re.sub(\"@\\S+\",\" \",x)\n",
    "\tx = re.sub(\"#\\S+\",\" \",x)\n",
    "\tx = re.sub(\"\\'\\w+\",\"\",x)\n",
    "\tx = re.sub(\"[%s]\" % re.escape(string.punctuation),\" \",x)\n",
    "\tx = re.sub(\"\\w*\\d+\\w*\",\"\",x)\n",
    "\tx = re.sub(\"\\s{2,}\",\" \",x)\n",
    "\treturn x\n",
    "\t\n",
    "temp = []\n",
    "data_to_list = df[\"tweet\"].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "\ttemp.append(preprocess(data_to_list[i]))\n",
    "\t\n",
    "def tokenize(y):\n",
    "\tfor x in y:\n",
    "\t\tyield(word_tokenize(str(x)))\n",
    "\t\t\n",
    "data_words = list(tokenize(temp))\n",
    "\n",
    "def detokenize(txt):\n",
    "\treturn TreebankWordDetokenizer().detokenize(txt)\n",
    "\t\n",
    "final_data = []\n",
    "for i in range(len(data_words)):\n",
    "\tfinal_data.append(detokenize(data_words[i]))\n",
    "print(final_data[:5])\n",
    "final_data = np.array(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9cd938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...     5    49    55]\n",
      " [    0     0     0 ...    12     5   511]\n",
      " [    0     0     0 ...  1039    72    45]\n",
      " ...\n",
      " [    0     0     0 ...   341    28   269]\n",
      " [    0     0     0 ...  1800     6  1296]\n",
      " [    0     0     0 ...   115     2 18599]]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 200, 64)           37248     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                24960     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 2,622,403\n",
      "Trainable params: 2,622,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "436/436 [==============================] - 14s 26ms/step - loss: 0.4052 - accuracy: 0.8580 - val_loss: 0.2979 - val_accuracy: 0.8961\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89606, saving model to model_gru.hdf5\n",
      "Epoch 2/5\n",
      "436/436 [==============================] - 11s 25ms/step - loss: 0.2091 - accuracy: 0.9293 - val_loss: 0.3153 - val_accuracy: 0.8840\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89606\n",
      "Epoch 3/5\n",
      "436/436 [==============================] - 11s 24ms/step - loss: 0.1279 - accuracy: 0.9569 - val_loss: 0.3674 - val_accuracy: 0.8819\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89606\n",
      "Epoch 4/5\n",
      "436/436 [==============================] - 11s 25ms/step - loss: 0.0820 - accuracy: 0.9726 - val_loss: 0.4427 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89606\n",
      "Epoch 5/5\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.4826 - val_accuracy: 0.8707\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89606\n",
      "194/194 - 2s - loss: 0.2927 - accuracy: 0.8975\n",
      "194/194 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Test acc: 89.75 %\n",
      "Test loss: 29.27 %\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_words = 20000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(final_data)\n",
    "sequences = tokenizer.texts_to_sequences(final_data)\n",
    "tweets = pad_sequences(sequences,maxlen=max_len)\n",
    "with open(\"tokenizer.pickle\",\"wb\") as handle:\n",
    "\tpickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(tweets)\n",
    "\n",
    "labels = df[\"class\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(tweets,labels,random_state=42)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.25,random_state=42)\n",
    "\n",
    "inputs = tf.keras.Input(shape=max_len,dtype=\"int32\")\n",
    "x = layers.Embedding(max_words,128)(inputs)\n",
    "x = layers.GRU(64,return_sequences=True)(x)\n",
    "x = layers.GRU(64)(x)\n",
    "outputs = layers.Dense(3,activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "checkpoint = ModelCheckpoint(\"model_gru.hdf5\",monitor=\"val_accuracy\",verbose=1,save_best_only=True,save_weights_only=False)\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=5,validation_data=(x_val,y_val),callbacks=[checkpoint])\n",
    "best = tf.keras.models.load_model(\"model_gru.hdf5\")\n",
    "loss,acc = best.evaluate(x_test,y_test,verbose=2)\n",
    "pred = best.evaluate(x_test)\n",
    "print(\"Test acc: {:.2f} %\".format(100*acc))\n",
    "print(\"Test loss: {:.2f} %\".format(100*loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45e272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
